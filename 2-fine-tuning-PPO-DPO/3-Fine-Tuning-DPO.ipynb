{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a93179",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge pytorch=2.2.0 torchvision=0.17.0 torchtext=0.17.2 cpuonly numpy pandas matplotlib scikit-learn tqdm -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b694e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets==2.20.0 trl==0.9.6 transformers==4.42.3 peft==0.11.1 sacrebleu==2.4.2 evaluate==0.4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df3a615",
   "metadata": {},
   "source": [
    " # Preference Tuning with DPO\n",
    "\n",
    "When we use the Hugging Face stack, preference tuning is eerily similar to\n",
    "the instruction tuning we covered before with some slight differences. We\n",
    "will still be using TinyLlama but this time an instruction-tuned version that\n",
    "was first trained using full fine-tuning and then further aligned with DPO.\n",
    "Compared to our initial instruction-tuned model, this LLM was trained on\n",
    "much larger datasets.\n",
    "In this section, we will demonstrate how you can further align this model\n",
    "using DPO with reward-based datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9ff9240",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chosen', 'rejected', 'prompt'],\n",
       "    num_rows: 5922\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLLama\n",
    "    is using\"\"\"\n",
    "    # Format answers\n",
    "    system = \"<|system|>\\n\" + example[\"system\"] + \"</s>\\n\"\n",
    "    prompt = \"<|user|>\\n\" + example[\"input\"] + \" </s>\\n<|assistant|>\\n\"\n",
    "    chosen = example[\"chosen\"] + \"</s>\\n\"\n",
    "    rejected = example[\"rejected\"] + \"</s>\\n\"\n",
    "    return {\n",
    "        \"prompt\": system + prompt,\n",
    "        \"chosen\": chosen,\n",
    "        \"rejected\": rejected,\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply formatting to the dataset and select relatively short answers\n",
    "dpo_dataset = load_dataset(\"argilla/distilabel-intel-orca-dpo-pairs\", split=\"train\")\n",
    "dpo_dataset = dpo_dataset.filter(\n",
    "    lambda r: \n",
    "        r[\"status\"] != \"tie\"\n",
    "        and r[\"chosen_score\"] >= 8\n",
    "        and not r[\"in_gsm8k_train\"]\n",
    ")\n",
    "dpo_dataset = dpo_dataset.map(format_prompt, remove_columns=dpo_dataset.column_names)\n",
    "dpo_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0440fee0",
   "metadata": {},
   "source": [
    "Note that we apply additional filtering to further reduce the size of the data\n",
    "to roughly 6,000 examples from the original 13,000 examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9afb253",
   "metadata": {},
   "source": [
    "# Model Quantization\n",
    "\n",
    "We load our base model and load it with the LoRA we created previously.\n",
    "As before, we quantize the model to reduce the necessary VRAM for\n",
    "training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7124a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BitsAndBytesConfig,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "# 4-bit quantization configuration - Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\",  # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\",  # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True,  # Apply nested quantization\n",
    ")\n",
    "\n",
    "# Load LLaMA tokenizer\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\"\n",
    "\n",
    "# Merge LoRA and base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=bnb_config,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95455f29",
   "metadata": {},
   "source": [
    "Next, we use the same LoRA configuration as before to perform the DPO\n",
    "training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5a96c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "\n",
    "# Prepare LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.05,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[  # Layers to target\n",
    "        \"k_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"v_proj\",\n",
    "        \"up_proj\",\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"down_proj\",\n",
    "    ],\n",
    ")\n",
    "# prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a355b9bc",
   "metadata": {},
   "source": [
    "# Training Configuration\n",
    "\n",
    "For the sake of simplicity, we will use the same training arguments as we\n",
    "did before with one difference. Instead of running for a single epoch (which\n",
    "can take up to two hours), we run for 200 steps instead for illustration\n",
    "purposes. Moreover, we added the warmup_ratio parameter, which\n",
    "increases the learning rate from 0 to the learning_rate value we set for\n",
    "the first 10% of steps. By maintaining a small learning rate at the start (i.e.,warmup period), we allow the model to adjust to the data before applying\n",
    "larger learning rates, therefore avoiding harmful divergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6960ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DPOConfig\n",
    "\n",
    "output_dir = \"./dpo-tinyllama\"\n",
    "\n",
    "# Training arguments\n",
    "training_arguments = DPOConfig(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=1e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=200,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio=0.1,\n",
    "    beta=0.1,\n",
    "    label_smoothing=0.0,\n",
    "    loss_type=\"sigmoid\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0a9686",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Now that we have prepared all our models and parameters, we can start\n",
    "fine-tuning our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27e7383",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/anaconda3/envs/torch-env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_prompt_length, max_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in DPOTrainer, please use the DPOConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/pedro/anaconda3/envs/torch-env/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n",
      "/home/pedro/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:675: UserWarning: You passed `max_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/pedro/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:688: UserWarning: You passed `max_prompt_length` to the DPOTrainer, the value you passed will override the one in the `DPOConfig`.\n",
      "  warnings.warn(\n",
      "/home/pedro/.local/lib/python3.10/site-packages/trl/trainer/dpo_trainer.py:728: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "Tokenizing train dataset:   0%|          | 0/5922 [00:00<?, ? examples/s]Token indices sequence length is longer than the specified maximum sequence length for this model (2055 > 2048). Running this sequence through the model will result in indexing errors\n",
      "Tokenizing train dataset: 100%|██████████| 5922/5922 [00:21<00:00, 278.13 examples/s]\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer\n",
    "\n",
    "# Create DPO trainer\n",
    "dpo_trainer = DPOTrainer(\n",
    "    model, \n",
    "    ref_model=None, # The reference model (not used in this case because LoRA has been used)\n",
    "    args=training_arguments,\n",
    "    train_dataset=dpo_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    "    beta=0.1,\n",
    "    max_prompt_length=512,\n",
    "    max_length=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ac5a594",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/200 [00:00<?, ?it/s]/home/pedro/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/home/pedro/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:90: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  5%|▌         | 10/200 [01:05<20:09,  6.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6913, 'grad_norm': 2.3933210372924805, 'learning_rate': 5e-06, 'rewards/chosen': 0.00039761661901138723, 'rewards/rejected': -0.00327131524682045, 'rewards/accuracies': 0.30000001192092896, 'rewards/margins': 0.0036689317785203457, 'logps/rejected': -91.34730529785156, 'logps/chosen': -83.61310577392578, 'logits/rejected': -3.0855438709259033, 'logits/chosen': -3.0500998497009277, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 20/200 [02:10<19:56,  6.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.673, 'grad_norm': 2.6381709575653076, 'learning_rate': 1e-05, 'rewards/chosen': -0.003814463736489415, 'rewards/rejected': -0.046028126031160355, 'rewards/accuracies': 0.4625000059604645, 'rewards/margins': 0.04221365973353386, 'logps/rejected': -130.34994506835938, 'logps/chosen': -98.92628479003906, 'logits/rejected': -3.1642000675201416, 'logits/chosen': -3.0853583812713623, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 30/200 [03:18<19:03,  6.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.635, 'grad_norm': 2.0923023223876953, 'learning_rate': 9.924038765061042e-06, 'rewards/chosen': -0.02209140732884407, 'rewards/rejected': -0.1545577049255371, 'rewards/accuracies': 0.4625000059604645, 'rewards/margins': 0.13246631622314453, 'logps/rejected': -115.07453918457031, 'logps/chosen': -81.56785583496094, 'logits/rejected': -3.1240222454071045, 'logits/chosen': -3.073723316192627, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 40/200 [04:14<15:50,  5.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.583, 'grad_norm': 1.7409669160842896, 'learning_rate': 9.727592877996585e-06, 'rewards/chosen': -0.05093228071928024, 'rewards/rejected': -0.35245656967163086, 'rewards/accuracies': 0.512499988079071, 'rewards/margins': 0.30152428150177, 'logps/rejected': -132.6724853515625, 'logps/chosen': -96.96385955810547, 'logits/rejected': -3.091298818588257, 'logits/chosen': -3.069164991378784, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 50/200 [05:21<15:55,  6.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5904, 'grad_norm': 3.753676176071167, 'learning_rate': 9.37309853569698e-06, 'rewards/chosen': -0.11603733152151108, 'rewards/rejected': -0.4164137840270996, 'rewards/accuracies': 0.48750001192092896, 'rewards/margins': 0.30037641525268555, 'logps/rejected': -133.6261749267578, 'logps/chosen': -104.56297302246094, 'logits/rejected': -3.1330058574676514, 'logits/chosen': -3.0932929515838623, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 60/200 [06:28<16:12,  6.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5998, 'grad_norm': 3.7886803150177, 'learning_rate': 8.885729807284855e-06, 'rewards/chosen': -0.10002808272838593, 'rewards/rejected': -0.48056063055992126, 'rewards/accuracies': 0.3499999940395355, 'rewards/margins': 0.38053256273269653, 'logps/rejected': -103.0716781616211, 'logps/chosen': -82.37723541259766, 'logits/rejected': -3.174006938934326, 'logits/chosen': -3.160590410232544, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 70/200 [07:35<14:42,  6.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5911, 'grad_norm': 1.6772764921188354, 'learning_rate': 8.280295144952537e-06, 'rewards/chosen': -0.1612970530986786, 'rewards/rejected': -0.6325763463973999, 'rewards/accuracies': 0.3499999940395355, 'rewards/margins': 0.4712792932987213, 'logps/rejected': -113.21795654296875, 'logps/chosen': -86.12324523925781, 'logits/rejected': -3.2088027000427246, 'logits/chosen': -3.185551166534424, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 80/200 [08:43<14:10,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5207, 'grad_norm': 2.0901639461517334, 'learning_rate': 7.575190374550272e-06, 'rewards/chosen': -0.14294259250164032, 'rewards/rejected': -0.889349102973938, 'rewards/accuracies': 0.48750001192092896, 'rewards/margins': 0.7464064359664917, 'logps/rejected': -139.4697723388672, 'logps/chosen': -108.7026596069336, 'logits/rejected': -3.104055404663086, 'logits/chosen': -3.0495753288269043, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 90/200 [09:50<11:58,  6.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.533, 'grad_norm': 4.759432792663574, 'learning_rate': 6.7918397477265e-06, 'rewards/chosen': -0.1827918142080307, 'rewards/rejected': -0.9802249670028687, 'rewards/accuracies': 0.4375, 'rewards/margins': 0.7974331974983215, 'logps/rejected': -148.990234375, 'logps/chosen': -91.01962280273438, 'logits/rejected': -3.1768617630004883, 'logits/chosen': -3.1311748027801514, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 100/200 [10:57<11:39,  6.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6635, 'grad_norm': 2.0014357566833496, 'learning_rate': 5.954044976882725e-06, 'rewards/chosen': -0.3962658941745758, 'rewards/rejected': -1.1358253955841064, 'rewards/accuracies': 0.4625000059604645, 'rewards/margins': 0.7395597696304321, 'logps/rejected': -154.74903869628906, 'logps/chosen': -100.98489379882812, 'logits/rejected': -3.1616950035095215, 'logits/chosen': -3.1237246990203857, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 110/200 [12:05<10:08,  6.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4963, 'grad_norm': 1.7281523942947388, 'learning_rate': 5.087262032186418e-06, 'rewards/chosen': -0.21976308524608612, 'rewards/rejected': -1.3686443567276, 'rewards/accuracies': 0.5375000238418579, 'rewards/margins': 1.148881196975708, 'logps/rejected': -166.9852752685547, 'logps/chosen': -115.24952697753906, 'logits/rejected': -3.1242799758911133, 'logits/chosen': -3.0581512451171875, 'epoch': 0.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 120/200 [13:17<09:35,  7.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5204, 'grad_norm': 6.5013532638549805, 'learning_rate': 4.217827674798845e-06, 'rewards/chosen': -0.2142508327960968, 'rewards/rejected': -1.2666479349136353, 'rewards/accuracies': 0.512499988079071, 'rewards/margins': 1.0523971319198608, 'logps/rejected': -162.53964233398438, 'logps/chosen': -94.13519287109375, 'logits/rejected': -3.089872360229492, 'logits/chosen': -2.995034694671631, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 130/200 [14:18<07:13,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.581, 'grad_norm': 2.537200927734375, 'learning_rate': 3.372159227714218e-06, 'rewards/chosen': -0.2623863220214844, 'rewards/rejected': -0.9639021754264832, 'rewards/accuracies': 0.4124999940395355, 'rewards/margins': 0.7015158534049988, 'logps/rejected': -120.1742935180664, 'logps/chosen': -96.2881851196289, 'logits/rejected': -3.133657932281494, 'logits/chosen': -3.115107297897339, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 140/200 [15:24<07:02,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5806, 'grad_norm': 7.107076644897461, 'learning_rate': 2.5759518987683154e-06, 'rewards/chosen': -0.22886653244495392, 'rewards/rejected': -1.026235818862915, 'rewards/accuracies': 0.4124999940395355, 'rewards/margins': 0.7973693013191223, 'logps/rejected': -114.10166931152344, 'logps/chosen': -101.51978302001953, 'logits/rejected': -3.173781156539917, 'logits/chosen': -3.1216189861297607, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 150/200 [16:26<05:12,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5813, 'grad_norm': 0.8339665532112122, 'learning_rate': 1.8533980447508138e-06, 'rewards/chosen': -0.17669036984443665, 'rewards/rejected': -1.162750244140625, 'rewards/accuracies': 0.38749998807907104, 'rewards/margins': 0.986059844493866, 'logps/rejected': -118.98201751708984, 'logps/chosen': -57.67116165161133, 'logits/rejected': -3.0821642875671387, 'logits/chosen': -3.0392024517059326, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 160/200 [17:30<04:22,  6.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5835, 'grad_norm': 2.645308256149292, 'learning_rate': 1.22645209888614e-06, 'rewards/chosen': -0.2559385895729065, 'rewards/rejected': -1.1220470666885376, 'rewards/accuracies': 0.42500001192092896, 'rewards/margins': 0.8661085367202759, 'logps/rejected': -122.52392578125, 'logps/chosen': -82.15196228027344, 'logits/rejected': -3.149512529373169, 'logits/chosen': -3.1040146350860596, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 170/200 [18:35<03:22,  6.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5778, 'grad_norm': 2.8055620193481445, 'learning_rate': 7.141634964894389e-07, 'rewards/chosen': -0.15658487379550934, 'rewards/rejected': -0.9607060551643372, 'rewards/accuracies': 0.3499999940395355, 'rewards/margins': 0.804121196269989, 'logps/rejected': -107.6840591430664, 'logps/chosen': -76.82438659667969, 'logits/rejected': -3.122872829437256, 'logits/chosen': -3.1170833110809326, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 180/200 [19:38<02:01,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6247, 'grad_norm': 5.650725364685059, 'learning_rate': 3.320978675139919e-07, 'rewards/chosen': -0.3278573155403137, 'rewards/rejected': -1.231153964996338, 'rewards/accuracies': 0.4124999940395355, 'rewards/margins': 0.9032966494560242, 'logps/rejected': -126.82087707519531, 'logps/chosen': -102.22705841064453, 'logits/rejected': -3.1422648429870605, 'logits/chosen': -3.146191120147705, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 190/200 [20:39<01:01,  6.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6626, 'grad_norm': 6.409192085266113, 'learning_rate': 9.186408276168012e-08, 'rewards/chosen': -0.24232156574726105, 'rewards/rejected': -0.5975373387336731, 'rewards/accuracies': 0.32499998807907104, 'rewards/margins': 0.35521575808525085, 'logps/rejected': -101.39582824707031, 'logps/chosen': -79.71857452392578, 'logits/rejected': -3.0484092235565186, 'logits/chosen': -3.01442551612854, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [21:42<00:00,  6.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5317, 'grad_norm': 3.9176714420318604, 'learning_rate': 7.615242180436521e-10, 'rewards/chosen': -0.16919878125190735, 'rewards/rejected': -1.207228183746338, 'rewards/accuracies': 0.4625000059604645, 'rewards/margins': 1.0380291938781738, 'logps/rejected': -134.67039489746094, 'logps/chosen': -79.00543975830078, 'logits/rejected': -3.1448285579681396, 'logits/chosen': -3.0928685665130615, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [21:45<00:00,  6.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1305.2724, 'train_samples_per_second': 1.226, 'train_steps_per_second': 0.153, 'train_loss': 0.5910451936721802, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.5910451936721802, metrics={'train_runtime': 1305.2724, 'train_samples_per_second': 1.226, 'train_steps_per_second': 0.153, 'total_flos': 0.0, 'train_loss': 0.5910451936721802, 'epoch': 0.2701789935832489})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start fine tuning \n",
    "dpo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87848ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log = pd.DataFrame(dpo_trainer.state.log_history)\n",
    "log_t = log[log[\"loss\"].notna()]\n",
    "log_e = log[log[\"eval_loss\"].notna()]\n",
    "\n",
    "# Plot train and evaluation loss\n",
    "plt.plot(log_t[\"epoch\"], log_t[\"loss\"], label=\"Train\")\n",
    "plt.plot(log_e[\"epoch\"], log_e[\"eval_loss\"], label=\"Eval\")\n",
    "plt.title(\"Model Losses\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700c78da",
   "metadata": {},
   "source": [
    "### Notes\n",
    "\n",
    "Using `save_model()` we save the LoRA adapters alongside the models. This means that `output_dir` will contain both the model weights and LoRA adapters as different modules. \n",
    "\n",
    "The function `merge_and_unload()` create a standalone model by merging the model weights with the LoRA adapters. This step also free precious VRAM by unloading the LoRA adapters which are no longer necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2637d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the adapter \n",
    "dpo_trainer.save_model(output_dir)\n",
    "\n",
    "# Merge the base model weights to LoRA adapters\n",
    "# Also free vram by unloading LoRA adapters \n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4fa7c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/anaconda3/envs/torch-env/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"from peft import PeftModel\n",
    "\n",
    "# Load the model with the LoRA adapters active\n",
    "dpo_model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    output_dir,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "merged_model = dpo_model.merge_and_unload()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e2ace9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./dpo-tinyllama-merged/tokenizer_config.json',\n",
       " './dpo-tinyllama-merged/special_tokens_map.json',\n",
       " './dpo-tinyllama-merged/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "merged_dir = \"./dpo-tinyllama-merged\"\n",
    "\n",
    "merged_model.save_pretrained(merged_dir)\n",
    "tokenizer.save_pretrained(merged_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac630f10",
   "metadata": {},
   "source": [
    "# Testing inference\n",
    "\n",
    "Let's load our fine-tuned model as we would do in a production pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265e96a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain quantum computing in simple terms. Let's say you have to find the square roots of numbers. In classical computing, you could use a calculator to do this, but quantum computers can solve this problem in a fraction of the time. How does quantum computing work?\n",
      "\n",
      "A: Quantum computing works by using quantum bits, or qubits, which can take on both a 0 or a 1, and can exist in multiple states at the same time. A quantum computer can solve problems that are too computationally intensive for classical computers by using a technique called quantum entanglement.\n",
      "\n",
      "When two qubits are entangled, they can interact with each other in ways that are not possible for classical bits. If two qubits are entangled, they can perform a quantum computation by acting on one qubit simultaneously with the other qubit. This means that if one qubit performs a calculation, the other qubit can act on it simultaneously, either with a 0 or with a 1, and the result will be the same.\n",
      "\n",
      "In classical computers, the qubits need to be wiped clean between calculations. In a quantum computer, this is not necessary. When one qubit performs a calculation, all of the other qubits are wip\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "model_dir = \"./dpo-tinyllama-merged\"\n",
    "\n",
    "finetuned_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_dir,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "finetuned_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_dir,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=finetuned_model,\n",
    "    tokenizer=finetuned_tokenizer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    do_sample=True,\n",
    ")\n",
    "\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c516b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
