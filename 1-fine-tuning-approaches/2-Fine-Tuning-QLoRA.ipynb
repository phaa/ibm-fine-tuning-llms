{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2653eb5",
   "metadata": {},
   "source": [
    "#  Instruction-Tuning with LLMs\n",
    "\n",
    "\n",
    "Instruction-based fine-tuning, referred to as instruction LLAMA. It trains the language models to follow specific instructions and generate appropriate responses. For instruction-tuning, the dataset plays an important role as it provides structured examples of instructions, contexts, and responses, allowing the model to learn how to handle various tasks effectively. Instruction LLAMA often uses human feedback to refine and improve model performance; however, this lab doesn't cover this aspect.\n",
    "\n",
    "The context and instruction are concatenated to form a single input sequence that the model can understand and use to generate the correct response.\n",
    "\n",
    "#### Context and instruction\n",
    "\n",
    "\t•\tInstruction: A command to specify what the model should do\n",
    "\t•\tContext: Additional information or background required for performing the instruction\n",
    "\t•\tCombined input: The instruction and context combine together into a single input sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a18f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "659721ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from datasets import load_dataset\n",
    "from peft import AutoPeftModelForCausalLM, LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf4a7fa",
   "metadata": {},
   "source": [
    "# Load a tokenizer and format function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab071841",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    ")\n",
    "\n",
    "def format_prompt(example):\n",
    "    \"\"\"Format the prompt to using the <|user|> template TinyLLama\n",
    "    is using\n",
    "    \"\"\"\n",
    "\n",
    "    # Format answers\n",
    "    chat = example[\"messages\"]\n",
    "    prompt = template_tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "\n",
    "    return {\"text\": prompt}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3508523d",
   "metadata": {},
   "source": [
    "### Load and format the data using the template TinyLLama is using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c4ac213",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "  load_dataset(\"HuggingFaceH4/ultrachat_200k\", split=\"test_sft\")\n",
    "    .shuffle(seed=42)\n",
    "    .select(range(3_000))\n",
    ")\n",
    "\n",
    "dataset = dataset.map(format_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3410b9",
   "metadata": {},
   "source": [
    "Example of formatted prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25e45fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Given the text: Knock, knock. Who’s there? Hike.\n",
      "Can you continue the joke based on the given text material \"Knock, knock. Who’s there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Sure! Knock, knock. Who's there? Hike. Hike who? Hike up your pants, it's cold outside!</s>\n",
      "<|user|>\n",
      "Can you tell me another knock-knock joke based on the same text material \"Knock, knock. Who's there? Hike\"?</s>\n",
      "<|assistant|>\n",
      "Of course! Knock, knock. Who's there? Hike. Hike who? Hike your way over here and let's go for a walk!</s>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(dataset[\"text\"][2576])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac08c51e",
   "metadata": {},
   "source": [
    "# Model Quantization\n",
    "\n",
    "Now that we have our data, we can start loading in our model. This is where\n",
    "we apply the Q in QLoRA, namely quantization. We use the\n",
    "`bitsandbytes` package to compress the pretrained model to a 4-bit\n",
    "representation.\n",
    "In BitsAndBytesConfig, you can define the quantization scheme. We\n",
    "follow the steps used in the original QLoRA paper and load the model in 4-\n",
    "bit (`load_in_4bit`) with a normalized float representation\n",
    "(`bnb_4bit_quant_type`) and double quantization\n",
    "(`bnb_4bit_use_double_quant`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "628408c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"TinyLlama/TinyLlama-1.1B-intermediate-step-1431k-3T\"\n",
    "\n",
    "# 4-bit quantization configuration - The Q in QLoRA\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # Use 4-bit precision model loading\n",
    "    bnb_4bit_quant_type=\"nf4\", # Quantization type\n",
    "    bnb_4bit_compute_dtype=\"float16\", # Compute dtype\n",
    "    bnb_4bit_use_double_quant=True, # Apply nested quantization\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3396851",
   "metadata": {},
   "source": [
    "### Load the model to train on the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5686046",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a8ec119",
   "metadata": {},
   "source": [
    "### Load LLaMA tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089981e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "tokenizer.pad_token = \"<PAD>\"\n",
    "tokenizer.padding_side = \"left\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423b2438",
   "metadata": {},
   "source": [
    "We use `padding_side = \"left\"` with TinyLlama (decoder-only model) to ensure that useful tokens are at the end of the sequence. This is important because these models generate text autoregressively and expect informative tokens to come last. With left padding, the attention mask correctly ignores `<PAD>` tokens on the left.\n",
    "\n",
    "This quantization procedure allows us to decrease the size of the original\n",
    "model while retaining most of the original weights’ precision. Loading the\n",
    "model now only uses ~1 GB VRAM compared to the ~4 GB of VRAM it\n",
    "would need without quantization. \n",
    "\n",
    "> Note that during fine-tuning, more\n",
    "VRAM will be necessary so it does not cap out on the ~1 GB VRAM\n",
    "needed to load the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f077e7",
   "metadata": {},
   "source": [
    "# LoRA Configuration\n",
    "Next, we will need to define our LoRA configuration using the peft\n",
    "library, which represents hyperparameters of the fine-tuning process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59653e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LoRA Configuration\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,  # LoRA Scaling\n",
    "    lora_dropout=0.1,  # Dropout for LoRA Layers\n",
    "    r=64,  # Rank\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[ \n",
    "        \"k_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"v_proj\",\n",
    "        \"up_proj\",\n",
    "        \"q_proj\",\n",
    "        \"o_proj\",\n",
    "        \"down_proj\",\n",
    "    ], # Layers to target \n",
    ")\n",
    "\n",
    "# Prepare model for training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9450b5a3",
   "metadata": {},
   "source": [
    "`r`\n",
    "This is the rank of the compressed matrices. \n",
    "Increasing this value will also increase the sizes of compressed\n",
    "matrices leading to less compression and thereby improved\n",
    "representative power. Values typically range between 4 and 64.\n",
    "\n",
    "`lora_alpha`\n",
    "Controls the amount of change that is added to the original weights. In\n",
    "essence, it balances the knowledge of the original model with that of the\n",
    "new task. A rule of thumb is to choose a value twice the size of `r`.\n",
    "\n",
    "`target_modules`\n",
    "Controls which layers to target. The LoRA procedure can choose to\n",
    "ignore specific layers, like specific projection layers. This can speed up\n",
    "training but reduce performance and vice versa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172893d2",
   "metadata": {},
   "source": [
    "# Training Configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a3404ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"./results\"\n",
    "\n",
    "# Training Arguments\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_32bit\",\n",
    "    learning_rate=2e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=10,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6325248e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8b9a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/anaconda3/envs/torch-env/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/home/pedro/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/pedro/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/home/pedro/.local/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "  0%|          | 0/375 [00:00<?, ?it/s]/home/pedro/anaconda3/envs/torch-env/lib/python3.10/site-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "  3%|▎         | 10/375 [00:36<21:56,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6705, 'grad_norm': 0.26160547137260437, 'learning_rate': 0.00019964928592495045, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 20/375 [01:13<21:30,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4756, 'grad_norm': 0.25835326313972473, 'learning_rate': 0.0001985996037070505, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 30/375 [01:49<21:11,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4511, 'grad_norm': 0.18799914419651031, 'learning_rate': 0.0001968583161128631, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 40/375 [02:26<20:39,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.488, 'grad_norm': 0.19733956456184387, 'learning_rate': 0.00019443763702374812, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 50/375 [03:03<19:39,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.478, 'grad_norm': 0.19109897315502167, 'learning_rate': 0.0001913545457642601, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 60/375 [03:39<19:00,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3903, 'grad_norm': 0.19982600212097168, 'learning_rate': 0.00018763066800438636, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 70/375 [04:15<18:17,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4949, 'grad_norm': 0.2272220402956009, 'learning_rate': 0.00018329212407100994, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 80/375 [04:51<17:42,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4499, 'grad_norm': 0.19912979006767273, 'learning_rate': 0.000178369345732584, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 90/375 [05:27<17:18,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4275, 'grad_norm': 0.2007320374250412, 'learning_rate': 0.00017289686274214118, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 100/375 [06:04<16:52,  3.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4041, 'grad_norm': 0.22838197648525238, 'learning_rate': 0.00016691306063588583, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 110/375 [06:41<16:08,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4145, 'grad_norm': 0.202215313911438, 'learning_rate': 0.0001604599114862375, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 120/375 [07:17<15:27,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.377, 'grad_norm': 0.18593859672546387, 'learning_rate': 0.00015358267949789966, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 130/375 [07:53<14:49,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3321, 'grad_norm': 0.19036248326301575, 'learning_rate': 0.00014632960351198618, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 140/375 [08:30<14:33,  3.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.497, 'grad_norm': 0.2050643414258957, 'learning_rate': 0.0001387515586452103, 'epoch': 0.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 150/375 [09:08<13:54,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3465, 'grad_norm': 0.2627912759780884, 'learning_rate': 0.00013090169943749476, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 160/375 [09:44<13:01,  3.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4115, 'grad_norm': 0.20871563255786896, 'learning_rate': 0.00012283508701106557, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 170/375 [10:20<12:20,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.454, 'grad_norm': 0.17071916162967682, 'learning_rate': 0.00011460830285624118, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 180/375 [10:56<11:45,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3244, 'grad_norm': 0.2169497162103653, 'learning_rate': 0.00010627905195293135, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████     | 190/375 [11:33<11:07,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4192, 'grad_norm': 0.1909182369709015, 'learning_rate': 9.790575801166432e-05, 'epoch': 0.51}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 200/375 [12:09<10:40,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4746, 'grad_norm': 0.19712603092193604, 'learning_rate': 8.954715367323468e-05, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 210/375 [12:45<09:51,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.404, 'grad_norm': 0.1976407915353775, 'learning_rate': 8.126186854142752e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 220/375 [13:21<09:16,  3.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.342, 'grad_norm': 0.19072221219539642, 'learning_rate': 7.310801793847344e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████▏   | 230/375 [13:58<08:51,  3.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3612, 'grad_norm': 0.1956234574317932, 'learning_rate': 6.51427952678185e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 240/375 [14:34<08:14,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3872, 'grad_norm': 0.1848766803741455, 'learning_rate': 5.7422070843492734e-05, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 250/375 [15:10<07:31,  3.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3536, 'grad_norm': 0.19254186749458313, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 260/375 [15:47<06:56,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3462, 'grad_norm': 0.19941206276416779, 'learning_rate': 4.2928643231556844e-05, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 270/375 [16:23<06:18,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4659, 'grad_norm': 0.19478286802768707, 'learning_rate': 3.6257601025131026e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▍  | 280/375 [16:59<05:37,  3.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4339, 'grad_norm': 0.20654813945293427, 'learning_rate': 3.0033665948663448e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 290/375 [17:34<05:01,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3874, 'grad_norm': 0.20368149876594543, 'learning_rate': 2.4300494434824373e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 300/375 [18:09<04:23,  3.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3763, 'grad_norm': 0.17710012197494507, 'learning_rate': 1.9098300562505266e-05, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 310/375 [18:45<03:55,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3951, 'grad_norm': 0.19326646625995636, 'learning_rate': 1.4463573983949341e-05, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 320/375 [19:23<03:28,  3.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4378, 'grad_norm': 0.1809847354888916, 'learning_rate': 1.042882397605871e-05, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 330/375 [20:00<02:46,  3.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3868, 'grad_norm': 0.19370228052139282, 'learning_rate': 7.022351411174866e-06, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 340/375 [20:37<02:08,  3.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3882, 'grad_norm': 0.1877432018518448, 'learning_rate': 4.268050246793276e-06, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 350/375 [21:13<01:30,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3135, 'grad_norm': 0.18830986320972443, 'learning_rate': 2.1852399266194314e-06, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 360/375 [21:50<00:54,  3.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4441, 'grad_norm': 0.19025741517543793, 'learning_rate': 7.885298685522235e-07, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 370/375 [22:27<00:18,  3.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4522, 'grad_norm': 0.19409486651420593, 'learning_rate': 8.771699011416168e-08, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [22:47<00:00,  3.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1367.461, 'train_samples_per_second': 2.194, 'train_steps_per_second': 0.274, 'train_loss': 1.416849353790283, 'epoch': 1.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_arguments,\n",
    "    max_seq_length=512,\n",
    "\n",
    "    # Leave this out for regular SFT\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save QLoRA weights\n",
    "QLORA_MODEL = \"TinyLlama-1.1B-qlora\"\n",
    "trainer.model.save_pretrained(QLORA_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8ed349",
   "metadata": {},
   "source": [
    "# Merge Weights\n",
    "\n",
    "After we have trained our QLoRA weights, we still need to combine them\n",
    "with the original weights to use them. We reload the model in 16 bits,\n",
    "instead of the quantized 4 bits, to merge the weights. Although the tokenizer\n",
    "was not updated during training, we save it to the same folder as the model\n",
    "for easier access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "157f4501",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama-1.1B-qlora\", # QLORA_MODEL\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Merge LoRA and base model\n",
    "merged_model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a11ec2",
   "metadata": {},
   "source": [
    "After merging the adapter with the base model, we can use it with the\n",
    "prompt template that we defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "01003b8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|user|>\n",
      "Tell me something about Large Language Models.</s>\n",
      "<|assistant|>\n",
      "Large Language Models (LLMs) are a type of artificial intelligence (AI) that can generate human-like language. They are trained on large amounts of data, including text, audio, and video, and are capable of generating complex and nuanced language.\n",
      "\n",
      "LLMs are used in a variety of applications, including natural language processing (NLP), machine translation, and chatbots. They can be used to generate text, speech, or images, and can be trained to understand different languages and dialects.\n",
      "\n",
      "One of the most significant applications of LLMs is in the field of natural language generation (NLG). LLMs can be used to generate text in a variety of languages, including English, French, and German. They can also be used to generate speech, such as in a chatbot or voice assistant.\n",
      "\n",
      "LLMs have the potential to revolutionize the way we communicate and interact with each other. They can help us create more engaging and personalized content, and they can also help us to communicate more effectively with people from different cultures and backgrounds.\n",
      "\n",
      "Overall, LLMs are a promising technology that has the potential to change the way we communicate and interact with each other. They are a game-changer in the field of AI and are expected to have a significant impact on our daily lives.\n"
     ]
    }
   ],
   "source": [
    "# Use our predefined prompt template\n",
    "prompt = \"\"\"<|user|>\n",
    "Tell me something about Large Language Models.</s>\n",
    "<|assistant|>\n",
    "\"\"\"\n",
    "\n",
    "# Run our instruction-tuned model\n",
    "pipe = pipeline(task=\"text-generation\", model=merged_model,\n",
    "tokenizer=tokenizer)\n",
    "print(pipe(prompt)[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9405aa34",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
